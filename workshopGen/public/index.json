[
{
	"uri": "/workshop/",
	"title": "Fiserv/THD OpsMan Deep Dive Workshop",
	"tags": [],
	"description": "",
	"content": " Welcome everyone to the first ever Fiserv/THD collaborative workshop. Ultimately the goal of today is foster the PCF community between our organizations and provide the opportunity to share our experiences*. That being said, here is a proposed agenda based on the various topics we\u0026rsquo;ve all kicked around.\nAgenda 9:00 - Hellos \u0026amp; Coffee While drinking coffee - BBL UP while(bbl) { discussion(RoundTable.class) } Second cup - Make a BOSH Lunch:Time - Lunch!? After Lunch - Look at a BOSH 1:30 - Internals of OpsMan Ft David and Amit After that - Gleaming the Cube Beer:30 - Tony tells us a story ?:?? - Broker all the Services If we have time - Someone talks about Credhub We hope you enjoy the day. And remember, this all works on my machine.\n* And possibly establish an inter-office Ping-Pong League\n"
},
{
	"uri": "/concepts/",
	"title": "Concepts",
	"tags": [],
	"description": "",
	"content": " Concepts Learn the basics, understand architectures, and fill-in the core-concepts behind the workshop.\n"
},
{
	"uri": "/demos/",
	"title": "Demos",
	"tags": [],
	"description": "",
	"content": " Demos Build architectures, and test the core-concepts behind the workshop.\n"
},
{
	"uri": "/demos/bbl-up/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Goal Use the bosh bootloader to stand up our control plane (bosh director, concourse, docker hub)\nDirector Setup $ mkdir workspace $ cd workspace  Go to here and follow the various instructions in the links\nAlso: brew install cloudfoundry/tap/credhub-cli Also: install Fly CLI\n$ mkdir bbl-workshop $ cd bbl-workshop  We will distribute the Azure service account ENVs to everyone. Export them then:\n$ bbl up $ eval \u0026quot;$(bbl print-env)\u0026quot; $ bosh login $ bosh cloud-config  Deploy Concourse Now that we have our bosh director installed, we can deploy the concourse server that will be responsible for pipelining our PCF foundation installations.\n$ bbl plan --lb-type concourse $ bbl up  The BOSH bootloader is very versitile, with a simple switch of the plan we can use it to set up a concourse BOSH deployment. Now we need to configure that instance.\n$ bosh upload-stemcell \u0026quot;https://bosh.io/d/stemcells/bosh-azure-hyperv-ubuntu-xenial-go_agent\u0026quot; $ export EXTERNAL_HOST=\u0026quot;$(bbl outputs | grep concourse_lb_ip | cut -d ' ' -f2)\u0026quot;  Also notice that as of this moment, we have uploaded two different types of stemcells. We will explore that concept later, probably after lunch, probably.\n$ mkdir concourse-deployment $ cd concourse-deployment  This next step may seem complicated, but we are just using inline commands to generate rather large .yml files without having to fuss with an editor. Simply copy and everything from cat to EOL.\ncat \u0026gt; vars.yml \u0026lt;\u0026lt;EOL external_host: \u0026quot;${EXTERNAL_HOST}\u0026quot; external_url: \u0026quot;https://${EXTERNAL_HOST}\u0026quot; network_name: 'private' web_network_name: 'private' web_vm_type: 'default' web_network_vm_extension: 'lb' db_vm_type: 'default' db_persistent_disk_type: '1GB' worker_vm_type: 'default' deployment_name: 'concourse' uaa_version: '66.0' uaa_sha1: '2848d9fb65d2d556a918e8045359cf469c241123' main_team_oauth_users: - 'admin' EOL  One more\u0026hellip;.\ncat \u0026gt; credhub.yml \u0026lt;\u0026lt;EOL # Add Variables for Passwords - type: replace path: /variables?/name=concourse_to_credhub_secret value: name: concourse_to_credhub_secret type: password - type: replace path: /variables?/name=credhub_encryption_password value: name: credhub_encryption_password type: password # Add CredHub Release - type: replace path: /releases/- value: name: credhub url: https://bosh.io/d/github.com/pivotal-cf/credhub-release?v=((credhub_version)) sha1: ((credhub_sha1)) version: ((credhub_version)) # Add CredHub client to UAA - type: replace path: /instance_groups/name=web/jobs/name=uaa/properties/uaa/clients? value: credhub_cli: override: true authorized-grant-types: password,refresh_token scope: credhub.read,credhub.write authorities: uaa.resource access-token-validity: 1200 refresh-token-validity: 3600 secret: \u0026quot;\u0026quot; concourse_to_credhub: override: true authorized-grant-types: client_credentials scope: \u0026quot;\u0026quot; authorities: credhub.read,credhub.write access-token-validity: 30 refresh-token-validity: 3600 secret: ((concourse_to_credhub_secret)) # Add CredHub Job to Web Instance Group - type: replace path: /instance_groups/name=web/jobs/- value: name: credhub release: credhub properties: credhub: port: 8844 authorization: acls: enabled: false authentication: uaa: url: \u0026quot;((external_url)):8443\u0026quot; verification_key: ((uaa-jwt.public_key)) ca_certs: - ((atc_tls.ca)) data_storage: type: postgres database: \u0026amp;credhub_db credhub username: *credhub_db password: \u0026amp;credhub_db_passwd ((credhub_db_password)) require_tls: false tls: certificate: ((atc_tls.certificate)) private_key: ((atc_tls.private_key)) ca_certificate: ((atc_tls.ca)) log_level: info encryption: keys: - provider_name: int key_properties: encryption_password: ((credhub_encryption_password)) active: true providers: - name: int type: internal # Add CredHub DB to Postgres job - type: replace path: /instance_groups/name=db/jobs/name=postgres/properties/databases/databases/- value: name: *credhub_db - type: replace path: /instance_groups/name=db/jobs/name=postgres/properties/databases/roles/- value: name: *credhub_db password: *credhub_db_passwd - type: replace path: /variables?/name=credhub_db_password? value: name: credhub_db_password type: password # Point ATC to CredHub - type: replace path: /instance_groups/name=web/jobs/name=atc/properties/credhub? value: url: ((external_url)):8844 client_id: concourse_to_credhub client_secret: ((concourse_to_credhub_secret)) tls: ca_cert: certificate: ((atc_tls.ca)) insecure_skip_verify: true # Extend wait times - type: replace path: /update/canary_watch_time value: 1000-360000 - type: replace path: /update/update_watch_time value: 1000-360000 EOL  Now we have our yaml config files. Let\u0026rsquo;s put them to good use.\n$ git clone https://github.com/concourse/concourse-bosh-deployment.git ../../concourse-bosh-deployment  This downloads the concourse bosh deployment into directory next to the bbl-workshop\n$ pushd ../../concourse-bosh-deployment/cluster  If you don\u0026rsquo;t know what pushd and popd do, ask Chris.\n bosh deploy -d concourse concourse.yml \\ -l ../versions.yml \\ -l ../../bbl-workshop/concourse-deployment/vars.yml \\ -o operations/add-main-team-oauth-users.yml \\ -o operations/uaa.yml \\ -o operations/uaa-generic-oauth-provider.yml \\ -o ../../bbl-workshop/concourse-deployment/credhub.yml \\ -o operations/privileged-http.yml \\ -o operations/privileged-https.yml \\ -o operations/tls.yml \\ -o operations/tls-vars.yml \\ -o operations/web-network-extension.yml  Once that\u0026rsquo;s finished use popd to return to our concourse-deployment directory we created a few steps ago and download the concourse manifest.\n$ bosh -d concourse manifest \u0026gt; concourse.yml  Register and Deploy Credhub $ credhub login $ export UAA_USERS_ADMIN_KEY=$(credhub find -n uaa_users_admin | head -n 2 | tail -n 1 | cut -d ' ' -f3) $ export UAA_USERS_ADMIN_PASSWORD=$(credhub get -n $UAA_USERS_ADMIN_KEY | tail -n 3 | head -n 1 | cut -d ' ' -f2) $ echo $UAA_USERS_ADMIN_PASSWORD  Copy the password and open https://${EXTERNAL_HOST} to login with admin/$UAA_USERS_ADMIN_PASSWORD\n$ fly -t bbl-workshop login -c https://${EXTERNAL_HOST} -k  Copy / paste this callback to somewhere safe. More cat to EOL copy/paste\ncat \u0026gt; hello-credhub.ybml \u0026lt;\u0026lt;EOL jobs: - name: hello-credhub plan: - do: - task: hello-credhub config: platform: linux image_resource: type: docker-image source: repository: ubuntu run: path: sh args: - -exc - | echo \u0026quot;Hello $WORLD_PARAM\u0026quot; params: WORLD_PARAM: {{hello}} EOL  Put that one to good use.\n$ fly -t bbl-workshop set-pipeline -p hello-credhub -c hello-credhub.yml $ fly -t bbl-workshop unpause-pipeline -p hello-credhub $ fly -t bbl-workshop trigger-job -j hello-credhub/hello-credhub -w  Now we need to set a serious of ENVs. BTW, hope you kept that admin password.\n$ env -u CREDHUB_PROXY -u CREDHUB_SERVER -u CREDHUB_CLIENT -u CREDHUB_SECRET -u CREDHUB_CA_CERT bash $ credhub login -s https://${EXTERNAL_HOST}:8844 -u admin -p $UAA_USERS_ADMIN_PASSWORD --skip-tls-validation $ credhub set -n /concourse/main/hello -t value -v MAIN-CREDHUB $ fly -t bbl-workshop trigger-job -j hello-credhub/hello-credhub -w  One last override and our work here is done\n$ credhub login -s https://$EXTERNAL_HOST:8844 -u admin -p $UAA_USERS_ADMIN_PASSWORD --skip-tls-validation $ credhub set -n /concourse/main/hello-credhub/hello -t value -v PIPE-CREDHUB $ fly -t bbl-workshop trigger-job -j hello-credhub/hello-credhub -w $ exit  Also #exit!\n"
},
{
	"uri": "/demos/deploy-opsman-pipeline/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Goal Utilizing Concourse and PCF Pipelines we are going to configure and deploy OpsMan to a new PCF Foundation.\nPCF Pipelines Download the latest PCF-Pipeline from PivNet. If you can\u0026rsquo;t access this\u0026hellip;raise your hand and wait quietly.\nExtract the .tgz file to our bbl-workshop directory and set our Azure storage container.\n$ export AZURE_STORAGE_ACCOUNT=$(bbl outputs | grep storage_account_name | cut -d \u0026quot; \u0026quot; -f2) $ export AZURE_STORAGE_ACCOUNT_KEY=$(az storage account keys list --account-name $AZURE_STORAGE_ACCOUNT| jq -r .[0].value) $ az storage container create --name terraformstate --account-name $AZURE_STORAGE_ACCOUNT  Pause, move into the PCF Azure version of the pipeline.\n$ cd pcf-pipelines/install-pcf/azure  Now get ready for a whole bunch of Credhub copy/pasta.\n$ env -u CREDHUB_PROXY -u CREDHUB_SERVER -u CREDHUB_CLIENT -u CREDHUB_SECRET -u CREDHUB_CA_CERT bash $ credhub login -s https://$EXTERNAL_HOST:8844 -u admin -p $UAA_USERS_ADMIN_PASSWORD --skip-tls-validation $ credhub set -n /concourse/main/azure_client_id -t value -v $BBL_AZURE_CLIENT_ID $ credhub set -n /concourse/main/azure_client_secret -t value -v $BBL_AZURE_CLIENT_SECRET $ credhub set -n /concourse/main/pivnet_token -t value -v \u0026lt;get your pivnet token from network.pivotal.io\u0026gt; $ credhub set -n /concourse/main/azure_storage_account_key -t value -v $AZURE_STORAGE_ACCOUNT_KEY $ credhub generate -n /concourse/main/opsman_admin_password -t password $ credhub generate -n /concourse/main/opsman_ssh -t ssh -m ubuntu  Trivia: We debated letting you guys figure this part out on your own. I vetoed that. You all owe me.\nPretty Fly $ fly -t bbl-workshop set-pipeline -p install-pcf-azure -c pipeline.yml -l params.yml \\ --var \u0026quot;azure_client_id=((azure_client_id))\u0026quot; \\ --var \u0026quot;azure_client_secret=((azure_client_secret))\u0026quot; \\ --var \u0026quot;azure_region=$BBL_AZURE_REGION\u0026quot; \\ --var \u0026quot;azure_subscription_id=$BBL_AZURE_SUBSCRIPTION_ID\u0026quot; \\ --var \u0026quot;azure_storage_account_name=pcf\u0026quot; \\ --var \u0026quot;azure_buildpacks_container=buildpacks\u0026quot; \\ --var \u0026quot;azure_droplets_container=droplets\u0026quot; \\ --var \u0026quot;azure_packages_container=packages\u0026quot; \\ --var \u0026quot;azure_resources_container=resources\u0026quot; \\ --var \u0026quot;azure_tenant_id=$BBL_AZURE_TENANT_ID\u0026quot; \\ --var \u0026quot;azure_terraform_prefix=$(echo $AZURE_STORAGE_ACCOUNT | cut -c 7-20)\u0026quot; \\ --var \u0026quot;azure_vm_admin=ubuntu\u0026quot; \\ --var \u0026quot;company_name=MyCo\u0026quot; \\ --var \u0026quot;pcf_ert_domain=pcf.$(echo $AZURE_STORAGE_ACCOUNT | cut -c 7-20).io\u0026quot; \\ --var \u0026quot;system_domain=sys.pcf.$(echo $AZURE_STORAGE_ACCOUNT | cut -c 7-20).io\u0026quot; \\ --var \u0026quot;apps_domain=apps.pcf.$(echo $AZURE_STORAGE_ACCOUNT | cut -c 7-20).io\u0026quot; \\ --var \u0026quot;ert_major_minor_version=^2\\.3\\.[0-9]+$\u0026quot; \\ --var \u0026quot;opsman_admin_username=admin\u0026quot; \\ --var \u0026quot;opsman_admin_password=((opsman_admin_password))\u0026quot; \\ --var \u0026quot;opsman_domain_or_ip_address=opsman.pcf.$(echo $AZURE_STORAGE_ACCOUNT | cut -c 7-20).io\u0026quot; \\ --var \u0026quot;opsman_major_minor_version=^2\\.3\\.[0-9]+$\u0026quot; \\ --var \u0026quot;pcf_ssh_key_pub=((opsman_ssh.public_key))\u0026quot; \\ --var \u0026quot;pcf_ssh_key_priv=((opsman_ssh.private_key))\u0026quot; \\ --var \u0026quot;pivnet_token=((pivnet_token))\u0026quot; \\ --var \u0026quot;security_acknowledgement=X\u0026quot; \\ --var \u0026quot;azure_storage_container_name=terraformstate\u0026quot; \\ --var \u0026quot;terraform_azure_storage_access_key=((azure_storage_account_key))\u0026quot; \\ --var \u0026quot;terraform_azure_storage_account_name=$AZURE_STORAGE_ACCOUNT\u0026quot;  That was fun. Now that our concourse is properly configured we can use it to terraform and install opsman\n$ fly -t bbl-workshop unpause-pipeline -p install-pcf-azure $ fly -t bbl-workshop trigger-job -j install-pcf-azure/bootstrap-terraform-state -w  (Will seem to hang on unpack-tarball step for 15 min or so)\n$ fly -t bbl-workshop trigger-job -j install-pcf-azure/create-infrastructure -w  At this point, with out proper DNS and Domain\u0026rsquo;s defined, we cannot continue further. If anyone would like to donate a domain raise your hand and sit quietly.\n"
},
{
	"uri": "/demos/create-bosh-release/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Goal Deploying software systems with BOSH is done with BOSH Releases. Releases abstract code away from the underlying OS and create a specific packing structure all software systems must adhere to. Lets explore what a BOSH Release is by using a very simple release that prints log messages.\nCreate the BOSH Release To create the release directory, navigate into the workspace where you want the release to be, and run: - $ bosh init-release --dir \u0026lt;release_name\u0026gt;\nIf you have tree (Hint: $ brew install tree) you can view the directory structure:\ntree . . ├── config │ ├── blobs.yml │ └── final.yml ├── jobs ├── packages └── src 4 directories, 2 files  For this deployment we are going to make a very simple app that prints to STDOUT.\nCreate Source Code Inside the src folder make a new dir called sample_app\n$ cd src/ $ mkdir sample_app $ cd sample_app/  Using your favorite editor, create the app, add the source code for the script.\n#!/bin/bash while true do echo \u0026quot;Sample APP STDOUT!!!\u0026quot; sleep 1 done  Create the Job Navigate back to the release directory create the job skeleton - $ bosh generate-job sample_job\nOnce again tree will show use the new files and folders generated\ntree . . ├── config │ ├── blobs.yml │ └── final.yml ├── jobs │ └── sample_job │ ├── monit │ ├── spec │ └── templates ├── packages └── src └── sample_app └── app 7 directories, 5 files  A control script is used to tell the job how to start and stop. Navigate to the templates directory and create ctl.erb with an editor\n#!/bin/bash RUN_DIR=/var/vcap/sys/run/sample_job LOG_DIR=/var/vcap/sys/log/sample_job PIDFILE=${RUN_DIR}/pid case $1 in start) mkdir -p $RUN_DIR $LOG_DIR chown -R vcap:vcap $RUN_DIR $LOG_DIR echo $$ \u0026gt; $PIDFILE cd /var/vcap/packages/sample_app exec ./app \\ \u0026gt;\u0026gt; $LOG_DIR/sample_app.stdout.log \\ 2\u0026gt;\u0026gt; $LOG_DIR/sample_app.stderr.log ;; stop) kill -9 `cat $PIDFILE` rm -f $PIDFILE ;; *) echo \u0026quot;Usage: ctl {start|stop}\u0026quot; ;; esac  In order for BOSH to monitor the health of our running job it utilizes monit.\nTo configure monit for our process we create a monit file that explains what processid corresponds to our application we are asking to be monitored and how to start/stop the application.\ncheck process sample_app with pidfile /var/vcap/sys/run/sample_job/pid start program \u0026quot;/var/vcap/jobs/sample_job/bin/ctl start\u0026quot; stop program \u0026quot;/var/vcap/jobs/sample_job/bin/ctl stop\u0026quot; group vcap  BOSH requires a monit file for each job in a release. When developing a release, you can use an empty monit file to meet this requirement without having to first create a control script.\nAt compile time, BOSH transforms templates into files, which it then replicates on the job VMs. The template names and file paths are among the metadata for each job that resides in the job spec file.\nEdit the spec file:\n--- name: sample_job templates: ctl.erb: bin/ctl packages: - sample_app properties: {}  Our sample_job has a dependency on the sample_app so we will create the package to encapsulate that in the next section.\nCreate the Package Once again we navigate back to the release directory and run the generate command\n$ bosh generate-package sample_app  And tree\ntree . . ├── config │ ├── blobs.yml │ └── final.yml ├── jobs │ └── sample_job │ ├── monit │ ├── spec │ └── templates │ └── ctl.erb ├── packages │ └── sample_app │ ├── packaging │ └── spec └── src └── sample_app └── app 8 directories, 8 files  Let\u0026rsquo;s update the spec, since our app only uses it\u0026rsquo;s source code, we can use the globbing pattern \u0026lt;package_name\u0026gt;/**/* to deep-traverse the directory in src where the source code should reside.\n--- name: sample_app dependencies: [] files: - sample_app/**/*  Since this release only has a bash script which does not require compilation we will just copy the script into the compiled code location allow us to use later.\nset -e -x cp -a sample_app/* ${BOSH_INSTALL_TARGET}  We have all the pieces of our bosh release. We just need to use bosh to create and upload the release. We need to use \u0026ndash;force because by default BOSH requires a blob to already exist. We don\u0026rsquo;t need a blob for our sample_app. - $ bosh create-release --force\n Adding package 'sample_app/e48b68edccfc35fc9bbd1fe37b652fd3bb261cb9'... Adding job 'sample_job/5765370e9d13c6c9d8a2db5631251e74fed718e9'... Added package 'sample_app/e48b68edccfc35fc9bbd1fe37b652fd3bb261cb9' Added job 'sample_job/5765370e9d13c6c9d8a2db5631251e74fed718e9' Added dev release 'bosh-release/0+dev.1' Name bosh-release Version 0+dev.1 Commit Hash non-git Job Digest Packages sample_job/5765370e9d13c6c9d8a2db5631251e74fed718e9 98c11bd46321008ed0802a75f5d26d3d05addb62 sample_app 1 jobs Package Digest Dependencies sample_app/e48b68edccfc35fc9bbd1fe37b652fd3bb261cb9 e32a56fd1fb3fa819d18eda5f4342d7fac2c3cf0 - 1 packages Succeeded   $ bosh upload-release\n Using environment 'https://10.0.0.6:25555' as client 'admin' ########################################################## 100.00% 2.08 KiB/s 0s Task 21 Task 21 | 00:17:50 | Extracting release: Extracting release (00:00:00) Task 21 | 00:17:50 | Verifying manifest: Verifying manifest (00:00:00) Task 21 | 00:17:50 | Resolving package dependencies: Resolving package dependencies (00:00:00) Task 21 | 00:17:50 | Creating new packages: sample_app/e48b68edccfc35fc9bbd1fe37b652fd3bb261cb9 (00:00:00) Task 21 | 00:17:50 | Creating new jobs: sample_job/5765370e9d13c6c9d8a2db5631251e74fed718e9 (00:00:00) Task 21 | 00:17:50 | Release has been created: bosh-release/0+dev.1 (00:00:00) Task 21 Started Thu Nov 15 00:17:50 UTC 2018 Task 21 Finished Thu Nov 15 00:17:50 UTC 2018 Task 21 Duration 00:00:00 Task 21 done Succeeded  $ bosh releases\n Using environment 'https://10.0.0.6:25555' as client 'admin' Name Version Commit Hash bosh-dns 1.10.0* 7c6515f bosh-release 0+dev.1 non-git gogs 5.4.0* ca84293 (*) Currently deployed (+) Uncommitted changes 3 releases Succeeded   That\u0026rsquo;s it! You have now seen all the parts of a full BOSH release! Now that we have our release, we need to create a manifest to deploy it.\n"
},
{
	"uri": "/demos/deploy-bosh-release/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Goal Before we can deploy a BOSH release we need to define how the release should be deployed. Do we deploy 3 or 1 instances? Do we give it an IP of 10.0.0.1 or 10.0.0.50? etc\u0026hellip; As you can see there are many different ways to deploy the release. With BOSH we call this definition a BOSH Manifest. Lets create a manifest and deploy our sample-bosh-release.\nPart 1: Create the manifest Using your favorite command-line text editor to create a sample-bosh-manifest.yml text file.\nname: sample-bosh-deployment releases: - {name: bosh-release, version: latest} stemcells: - alias: trusty os: ubuntu-trusty version: latest instance_groups: - name: sample_vm instances: 1 networks: - name: default azs: [z1] jobs: - name: sample_job release: bosh-release stemcell: trusty vm_type: default update: canaries: 1 max_in_flight: 10 canary_watch_time: 1000-100000 update_watch_time: 1000-100000   The goal of this deployment is to deploy a single VM that runs our sample_job.\n name - the unique name of this deployment within a BOSH director. When a BOSH director receives subsequent deployment manifests with the same name it will assume it is an upgrade of the existing deployment.\n releases - lists the specific BOSH release versions that are to be used, ie the one we just made.\n instance_groups - lists the sets of instances (VMs) that will run the same job templates/packages as each other. Instance groups will be deployed as long running instances by default.\n stemcells - lists the stemcells (discussed earlier) we will run our instances with.\n  Part 2: Deploy!  Here is where all the magic happens! By BOSH deploying we combine the Stemcells, Releases, and Manifest to create our software system. This can take a few minutes.\n$ bosh -d sample-bosh-deployment deploy sample-bosh-manifest.yml\n If successful, you should see something similar to the following:\nTask 80 Task 80 | 18:00:44 | Preparing deployment: Preparing deployment (00:00:00) Task 80 | 18:00:44 | Preparing package compilation: Finding packages to compile (00:00:00) Task 80 | 18:00:44 | Compiling packages: sample_app/3698d0cc632d3162a6a2fedcd36ac00364a7cd64 (00:00:57) Task 80 | 18:04:39 | Creating missing vms: sample_vm/71160b1a-faaa-487c-b107-7d4ed8fce7ce (0) Task 80 | 18:05:28 | Updating instance sample_vm: sample_vm/71160b1a-faaa-487c-b107-7d4ed8fce7ce (0) (canary) Task 80 Started Mon Nov 27 18:00:44 UTC 2017 Task 80 Finished Mon Nov 27 18:05:46 UTC 2017 Task 80 Duration 00:05:02 Task 80 done Succeeded    "
},
{
	"uri": "/demos/example-stemcell/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Goal BOSH Stemcells contain both metadata and the raw image. Lets take some time to examine what is contained in the metadata and how a Stemcell is organized.\nPrerequisites  A file archiver to extract files with.\nWindows - 7 Zip\nMac/Linux - Tar - (Factory Installed)\n OPTIONAL - VirtualBox to boot a raw VMDK system image.\n  Part 1: Extracting the stemcell image  Download the latest vSphere Stemcell here Extract the downloaded stemcell\ntar -xvf bosh-stemcell-*-azure-hyperv-ubuntu-trusty-go_agent.tgz\n Take a look at the contents of the stemcell.MF file\ncat stemcell.MF\nNotice versioning, and specific metadata for the stemcell\n Take a look at the contents of the stemcell_dpkg_l.txt file\ncat stemcell_dpkg_l.txt\nNotice the utilities which come pre-installed on the stemcell\n  Part 2 (OPTIONAL): Booting the stemcell image The follow steps are OPTIONAL, and recommended only if you have VirtualBox installed.\n Rename the image file to image.tgz\nmv image image.tgz\n Extract the image.tgz file\ntar -xvf image.tgz\nNotice the raw operating system image in the vmdk format, since this is a vSphere Image.\n Launch a vm based on the image-disk1.vmdk inside VirtualBox\n Login with vcap and c1oudc0w\n  "
},
{
	"uri": "/demos/init/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Creating Your First Service Broker Get Service Broker Skeleton Project  Install Golang, setup $GOROOT. Golang has great documentation on how to do this here.\n Create a working directory to store our service broker in. We will use /tmp for the rest of this workshop.\nmkdir -p /tmp/service-broker  Set the working directory($GOPATH) for the go cli.\nexport GOPATH=$(pwd)  Get the skeleton service broker code using go get.\ngo get github.com/Pivotal-Field-Engineering/service-broker   Run the Service Broker  Run the service broker using go run.\ngo run src/github.com/Pivotal-Field-Engineering/service-broker/main.go   Note: By default the service broker runs on port 8080 and with basic authentication credentials of pivotal/pivotal\nView the Service Broker Catalog Choose one of the following:\n Using curl hit the /v2/catalog endpoint via curl -X GET -H 'X-Broker-API-Version:2.14' -u pivotal:pivotal localhost:8080/v2/catalog\n Using Postman\n AUTH_HEADER - Username: pivotal, Password: pivotal HEADER - X-Broker-API-Version: 2.14 HTTP TYPE - GET ADDRESS - http://localhost:8080/v2/catalog     "
},
{
	"uri": "/demos/catalog/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Adding a Service Catalog We are creating a new plan for Medium hypothetical service\nExtending the Catalog with Medium Plan  Change into the broker source directory\ncd src/github.com/Pivotal-Field-Engineering/service-broker/broker  Open broker.go into your text IDE. All of the service broker logic lives here.\n Locate the function called Services in broker.go. This function corresponds with the /v2/catalog endpoint.\n Locate the Plan smallPlan. We see the smallPlan being created as a golang Struct. Let us now create a mediumPlan struct to model a Medium service type.\n Place the following code below the smallPlan. This will be our new mediumPlan!\nmediumPlan := brokerapi.ServicePlan{ Description: \u0026quot;medium-plan\u0026quot;, Free: \u0026amp;truePointer, Name: \u0026quot;medium\u0026quot;, Bindable: \u0026amp;truePointer, ID: \u0026quot;medium-id\u0026quot;, }  At this point we have 2 plans. You can continue to add as many as desired.\n Great! Now we have new plans, but our service broker has no awareness of these plans. To plug the plans into the service broker we add our new plans to the existing ServicePlan struct.\ns := brokerapi.Service{ ID: \u0026quot;Pivotal\u0026quot;, Name: \u0026quot;Pivotal Service Plans\u0026quot;, Description: \u0026quot;Service Plan which gives us Pivotal\u0026quot;, Bindable: true, PlanUpdatable: true, Tags: []string{}, Plans: []brokerapi.ServicePlan{smallPlan, mediumPlan}, }  Notice the line Plans: []brokerapi.ServicePlan{smallPlan,mediumPlan}, which has registered the small and medium plans. Save the broker.go file and get ready to run.\n  Run the Service Broker  Run the service broker using go run.\ngo run src/github.com/Pivotal-Field-Engineering/service-broker/main.go   View the Service Broker Catalog Choose one of the following:\n Using curl hit the /v2/catalog endpoint via curl -X GET -H 'X-Broker-API-Version:2.14' -u pivotal:pivotal localhost:8080/v2/catalog\n Using Postman\n AUTH_HEADER - Username: pivotal, Password: pivotal HEADER - X-Broker-API-Version: 2.14 HTTP TYPE - GET ADDRESS - http://localhost:8080/v2/catalog  The response should represent an Open Service Broker Catalog similar to the following:\n{ \u0026quot;services\u0026quot;: [ { \u0026quot;id\u0026quot;: \u0026quot;Pivotal\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;Pivotal Service Plans\u0026quot;, \u0026quot;description\u0026quot;: \u0026quot;Service Plan which gives us Pivotal\u0026quot;, \u0026quot;bindable\u0026quot;: true, \u0026quot;plan_updateable\u0026quot;: true, \u0026quot;plans\u0026quot;: [ { \u0026quot;id\u0026quot;: \u0026quot;small-id\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;small\u0026quot;, \u0026quot;description\u0026quot;: \u0026quot;small-plan\u0026quot;, \u0026quot;free\u0026quot;: true, \u0026quot;bindable\u0026quot;: true }, { \u0026quot;id\u0026quot;: \u0026quot;medium-id\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;medium\u0026quot;, \u0026quot;description\u0026quot;: \u0026quot;medium-plan\u0026quot;, \u0026quot;free\u0026quot;: true, \u0026quot;bindable\u0026quot;: true } ] } ] }   "
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/",
	"title": "Pivotal Workshop",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]